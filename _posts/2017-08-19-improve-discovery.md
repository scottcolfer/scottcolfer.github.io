---
layout: post
title: "10 discovery experiments you can try to improve discovery"
date: 2017-08-19
---



Experiment 1: Can’t interview? Don’t start.

Data/User Research suggest that
Teams wait for weeks to carry out their first interviews with users because they have to develop a route to users. This means that the first few weeks of Discovery may have limited return on investment and the final weeks are over-loaded with interviews and do not have enough time for the team to draw conclusions before Discovery ends
So if we try
Starting Discovery when the first interview can take place within 3 days (and pausing Discovery when first interview can’t take place within 3 days)
And measure
Time taken for first interview
Time taken to complete Discovery
% of Discoveries requiring extensions
Confidence in Discovery insights
We should see this change
Better insights from Discovery (with ‘better’ defined as team and stakeholder confidence in these insights)

Experiment 2: Subject Matter Experts are valuable
Data/User Research suggest that
Subject Matter Experts being a core member of a Discovery team increase the value of the Discovery (and may reduce the time taken to complete Discovery)
So if we try
Increasing the involvement of Subject Matter Experts with the core Discovery team (where this is useful)
And measure
% of Discovery team ceremonies attended by stakeholders
Time take to carry out first user interview
Amount of time spent with users during Discovery
% of Discoveries that require a deadline extension
We should see this change
Greater confidence in outcomes of Discovery (and potentially shorter Discoveries)

Experiment 3: Don’t be digital
Data/User Research suggest that
Discoveries carried out by ‘Digital’ teams will tend to skew Discoveries towards digital solutions
So if we try
‘Digital Plus’ teams (better mix of ‘the business’/policy/analytical services, etc and ‘digital’)
And measure
% of Discoveries that lead to digital solutions
We should see this change
Greater balance of digital/non-digital solutions

Experiment 4: Lean Startup over Scrum
Data/User Research suggest that
‘Pure’ Scrum is not a great framework for Discovery and Alpha because it expects requirements and value to be defined up-front
So if we try
A hypothesis-driven approach as described in The Lean Startup
And measure
% of Discoveries that do not continue to Alpha
% of Discoveries that pivot the problem, or user, or both
% of Discoveries that recommend pausing until conditions are right for Alpha
% of Discoveries that suggest that ‘digital’ isn’t the best, or only solution space to explore in Alpha
We should see this change
Data-driven decisions in Discovery;teams able to share the hypotheses how they’ve tested to draw conclusions as to the value of a problem being solved.

Experiment 5: Share Discovery Guidance
Data/User Research suggest that
Dysfunction can appear during Discovery due to a lack of shared understanding of the value of Discovery, and the approach to Discovery
Members of Discovery teams may not have the same understanding 
Discovery teams and their Service Manager may not have the same understanding
Service areas and ‘the business’/stakeholders
So if we try
Sharing Discovery guidance
And measure
% of discovery team members, Service Managers and stakeholders that can consistently describe the value and approach to Discovery
User satisfaction with Discovery (with users being stakeholders/’the business’
We should see this change
Teams encounter less dysfunction due to uncertainty around the point of Discovery and the approach to Discovery, and stakeholders’ expectations around Discovery being met more consistently.

Experiment 6: Don’t reinvent the wheel
Data/User Research suggest that
Some of the core concepts of Discovery and Alpha (new to government) share more with Project Management concepts (familiar to government) than we expect
So if we try
Highlighting similarities between agile development and project management
Learning from Project Management
Sharing guidance on when project management is the best approach, and when working with agility is the best approach
And measure
% of ‘work’ that makes an informed choice in the best model (agile or project) based on the conditions in which it’s working 
We should see this change
The right approach is taken for the right ‘work’ because there is greater shared understanding of how the approaches work and the value of their differences.

Experiment 7: Show me the money
Data/User Research suggest that
Discovery teams would like greater accountability 
So if we try
Making Discovery teams aware of their budget and the rate at which they are spending it
And measure
The average cost of a Discovery
We should see this change
Average cost decreases

Experiment 8: What’s a Pre-Discovery?
Data/User Research suggest that
Pre-Discovery has become a common stage of development but has yet to be clearly defined so is used inconsistently
So if we try
Defining the value of a Pre-Discovery stage, and deciding if it should exist, and (if ‘yes’) applying it correctly
And measure
Number of pre-Discoveries that take place in the future versus in the past
% Discovery teams reporting that a pre-Discovery helped their Discovery
We should see this change
Pre-Discovery of more use to Discovery teams 
Or
Pre-Discovery of more use to Portfolio
Or
Pre-Discovery no longer used
Or
???
Finding it difficult to be specific on this one

Experiment 9: Clearer briefs
Data/User Research suggest that
Discovery teams can spend several days (sometimes over a week) clarifying the brief for a Discovery (which is sometimes given in a conversation)
So if we try
A consistent approach to Discovery briefs
And measure
Average time spent by a team defining the brief
We should see this change
Teams begin Discovery quicker

Experiment 10: Run regular Discovery retrospectives
Data/User Research suggest that
The questions used to carry out this retrospective worked well
So if we try
Team/Department questionnaires
And measure
Responses
We should see this change
Greater confidence in themes; better informed hypotheses
